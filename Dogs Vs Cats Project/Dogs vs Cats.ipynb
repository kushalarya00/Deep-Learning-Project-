{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade3a96d",
   "metadata": {},
   "source": [
    "### Extract all the files from zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7f54dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(r\"C:\\Users\\Arya\\Desktop\\Data cv\\dogs-vs-cats.zip\") as zipObj:\n",
    "    # Extract all the contents of zip file in current directory\n",
    "    zipObj.extractall()\n",
    "    \n",
    "with ZipFile(r\"train.zip\") as zipObj:\n",
    "    # Extract all the contents of zip file in current directory\n",
    "    zipObj.extractall()\n",
    "    \n",
    "with ZipFile(r\"test1.zip\") as zipObj:\n",
    "# Extract all the contents of zip file in current directory\n",
    "    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57b119",
   "metadata": {},
   "source": [
    "### Creating Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdef41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing os module\n",
    "import os\n",
    "dataset_home = 'dataset_dogs_vs_cats/'\n",
    "subdirs = ['train/', 'test/']\n",
    "for subdir in subdirs:\n",
    "    # create label sub directories\n",
    "    labeldirs = ['dogs/', 'cats/']\n",
    "    for labldir in labeldirs:\n",
    "        newdir = dataset_home + subdir + labldir\n",
    "        os.makedirs(newdir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18487d97",
   "metadata": {},
   "source": [
    "### Now change dataset to fit above format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b0a6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "from random import seed, random\n",
    "seed(1)\n",
    "\n",
    "# define ratio of pictures to use for validation\n",
    "val_ratio = 0.25\n",
    "\n",
    "# copy training dataset images into subdirectories\n",
    "src_directory = 'train'\n",
    "dataset_home = 'dataset_dogs_vs_cats/'\n",
    "for file in listdir(src_directory):\n",
    "    src = src_directory + '/' + file\n",
    "    dst_dir = 'train/'\n",
    "    if random() < val_ratio:\n",
    "        dst_dir = 'test/'\n",
    "    if file.startswith('cat'):\n",
    "        dst = dataset_home + dst_dir + 'cats/' + file\n",
    "        copyfile(src, dst)\n",
    "    elif file.startswith('dog'):\n",
    "        dst = dataset_home + dst_dir + 'dogs/' + file\n",
    "        copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe973f",
   "metadata": {},
   "source": [
    "### Prepare a model for transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9c00864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 31s 1us/step\n",
      "58900480/58889256 [==============================] - 31s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "model = VGG16(include_top = False, input_shape = (224,224,3))\n",
    "\n",
    "# mark loaded layers as not trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# add new classifier layers\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "class1 = Dense(128, activation = 'relu', kernel_initializer = 'he_uniform')(flat1)\n",
    "output = Dense(1, activation = 'sigmoid')(class1)\n",
    "\n",
    "# define new model\n",
    "model = Model(inputs = model.inputs, outputs = output )\n",
    "\n",
    "# compile model\n",
    "opt = SGD(learning_rate = 0.001, momentum = 0.9)\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169301c8",
   "metadata": {},
   "source": [
    "### Run the model using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f443b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18697 images belonging to 2 classes.\n",
      "Found 6303 images belonging to 2 classes.\n",
      "99/99 [==============================] - 10921s 110s/step - loss: 0.2623 - accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center = True)\n",
    "\n",
    "# specify imagenet mean values for centering\n",
    "datagen.mean = [123.68, 116.779, 103.939]\n",
    "\n",
    "# prepare iterator\n",
    "train_it = datagen.flow_from_directory('dataset_dogs_vs_cats/train/', class_mode = 'binary', batch_size = 64, target_size = (224, 224))\n",
    "test_it = datagen.flow_from_directory('dataset_dogs_vs_cats/test/', class_mode = 'binary', batch_size = 64, target_size = (224, 224))\n",
    "\n",
    "# fit the model\n",
    "history = model.fit_generator(train_it, steps_per_epoch = len(test_it), epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf80fc",
   "metadata": {},
   "source": [
    "### Plot the loss and accuracy And Save the plot to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2601ad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-4609c11445e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cross Entropy Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'g'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# plot accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# plot loss\n",
    "plt.subplot(211)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'], color = 'b', label = 'train')\n",
    "plt.plot(history.history['val_loss'], color = 'g', label = 'test')\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(211)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color = 'b', label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], color = 'g', label = 'test')\n",
    "\n",
    "# save plot to file\n",
    "filename = sys.argv[0].split('/')[-1]\n",
    "plt.savefig(filename + '_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070dabe6",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace791cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('dogandcat_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfea5d",
   "metadata": {},
   "source": [
    "### Predict the output from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd2f0cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load and prepare the image\n",
    "# load the image\n",
    "model = load_model('dogandcat_model.h5')\n",
    "img = load_img(r\"C:\\Users\\Arya\\dataset_dogs_vs_cats\\test\\dogs\\dog.4.jpg\", target_size = (224, 224))\n",
    "\n",
    "# convert to array\n",
    "img = img_to_array(img)\n",
    "\n",
    "# reshape into a single sample with 3 channels\n",
    "img = img.reshape(1, 224, 224, 3)\n",
    "\n",
    "# center pixel data\n",
    "img = img.astype('float32')\n",
    "img = img - [123.68, 116.779, 103.939]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d099428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "# 1 is for dogs and 0 for cats\n",
    "result = model.predict(img)\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ee445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
